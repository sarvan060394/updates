Objective:

Implement two applications in OpenShift:
A producer application that sends events to a Kafka topic when a backing service is online.
A consumer application with 5 replicas that listens to the Kafka topic and restarts itself upon receiving an event.
Kafka Producer Application:

Script:
Create a producer.py script that sends events to a Kafka topic.
The script checks the status of the backing service and sends an event when the service is online.
The script uses confluent_kafka library.
Dockerfile:
Create a Dockerfile to containerize the producer script.
Deployment:
Create an OpenShift deployment YAML file (producer-deployment.yaml) for the producer application.
Steps:
Create producer.py:

python
Copy code
from confluent_kafka import Producer
import time

conf = {'bootstrap.servers': 'your_kafka_broker'}
producer = Producer(conf)

def delivery_report(err, msg):
    if err is not None:
        print('Message delivery failed: {}'.format(err))
    else:
        print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))

def send_event():
    producer.produce('test_topic', key='key', value='service_online', callback=delivery_report)
    producer.poll(1)

if __name__ == "__main__":
    while True:
        service_online = True  # Replace this with actual check
        if service_online:
            send_event()
            print("Event sent.")
            time.sleep(60)
----------------------------------------------------------------------
Kafka Consumer Application:

Script:
Create a consumer.py script that listens to Kafka events and restarts itself upon receiving an event.
The script uses confluent_kafka and Flask libraries.
Dockerfile:
Create a Dockerfile to containerize the consumer script.
Deployment:
Create an OpenShift deployment YAML file (consumer-deployment.yaml) for the consumer application with 5 replicas.
Steps:
Create consumer.py:

python
Copy code
from confluent_kafka import Consumer, KafkaError
import os
import sys
import threading
from flask import Flask

conf = {
    'bootstrap.servers': 'your_kafka_broker',
    'group.id': 'mygroup',
    'auto.offset.reset': 'earliest'
}

consumer = Consumer(conf)
consumer.subscribe(['test_topic'])

def restart_application():
    print("Restarting application...")
    os.execl(sys.executable, sys.executable, *sys.argv)

app = Flask(__name__)

@app.route('/restart', methods=['POST'])
def restart():
    threading.Thread(target=restart_application).start()
    return 'Service restarting...', 200

def listen_to_kafka():
    while True:
        msg = consumer.poll(1.0)
        if msg is None:
            continue
        if msg.error():
            if msg.error().code() == KafkaError._PARTITION_EOF:
                continue
            else:
                print(msg.error())
                break
        else:
            print(f'Received message: {msg.value().decode("utf-8")}')
            restart_application()

kafka_thread = threading.Thread(target=listen_to_kafka)
kafka_thread.start()

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
-----------------------------------------------------
This setup ensures that:

The producer application, running as a single replica, sends events to the Kafka topic whenever the backing service is online.
The consumer application, running as 5 replicas, listens for events on the Kafka topic and restarts itself upon receiving an event. Each replica restarts independently, ensuring the desired behavior without needing external authentication.
